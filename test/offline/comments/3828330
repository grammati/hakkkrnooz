<html><head><link rel="stylesheet" type="text/css" href="http://ycombinator.com/news.css">
<link rel="shortcut icon" href="http://ycombinator.com/favicon.ico">
<script>
function byId(id) {
  return document.getElementById(id);
}

function vote(node) {
  var v = node.id.split(/_/);   // {'up', '123'}
  var item = v[1]; 

  // hide arrows
  byId('up_'   + item).style.visibility = 'hidden';
  byId('down_' + item).style.visibility = 'hidden';

  // ping server
  var ping = new Image();
  ping.src = node.href;

  return false; // cancel browser nav
} </script><title>What is the asymptotic cost of purely functional programming? | Hacker News</title></head><body><center><table border=0 cellpadding=0 cellspacing=0 width="85%" bgcolor=#f6f6ef><tr><td bgcolor=#ff6600><table border=0 cellpadding=0 cellspacing=0 width="100%" style="padding:2px"><tr><td style="width:18px;padding-right:4px"><a href="http://ycombinator.com"><img src="http://ycombinator.com/images/y18.gif" width=18 height=18 style="border:1px #ffffff solid;"></img></a></td><td style="line-height:12pt; height:10px;"><span class="pagetop"><b><a href="news">Hacker News</a></b><img src="http://ycombinator.com/images/s.gif" height=1 width=10><a href="newest">new</a> | <a href="newcomments">comments</a> | <a href="ask">ask</a> | <a href="jobs">jobs</a> | <a href="submit">submit</a></span></td><td style="text-align:right;padding-right:4px;"><span class="pagetop"><a href="newslogin?whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">login</a></span></td></tr></table></td></tr><tr style="height:10px"></tr><tr><td><table border=0><tr><td><center><a id=up_3828330 href="vote?for=3828330&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3828330></span></center></td><td class="title"><a href="http://stackoverflow.com/q/1990464/69755">What is the asymptotic cost of purely functional programming?</a><span class="comhead"> (stackoverflow.com) </span></td></tr><tr><td colspan=1></td><td class="subtext"><span id=score_3828330>142 points</span> by <a href="user?id=lambda">lambda</a> 17 hours ago  | <a href="item?id=3828330">64 comments</a></td></tr><tr style="height:10px"></tr><tr><td></td><td><form method=post action="/r"><input type=hidden name="fnid" value="UaN6pJ7yB3"><textarea name="text" rows=6 cols=60></textarea><br><br>
<input type=submit value="add comment"></form></td></tr></table><br><br>
<table border=0><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=0></td><td valign=top><center><a id=up_3829039 href="vote?for=3829039&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829039></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=lmkg">lmkg</a> 15 hours ago  | <a href="item?id=3829039">link</a></span></div><br>
<span class="comment"><font color=#000000>The top-voted answer mentions that the example O(n log n) algorithm can be made O(n) by replacing eager evaluation with lazy evaluation. My question is, does this count as using the same algorithm or not?<p>I sort of view lazily-evaluated data structures (like generator expressions) as not data structures at all, but rather control flow constructs[1]. They control when, how, and if expressions are evaluated and results are returned. As a result, I feel that using lazy equivalents for your data structures qualifies as an algorithmic change. In fact, one could manually code lazy data access in an eager language, and the resulting algorithm would look very different from the original.<p>On the other hand, the laziness is enabled by the purity of the algorithm, so... well, it depends on your perspective. You could say that laziness makes the functional equivalent just as fast, or you could say that the functional equivalent is slower, but there's a different functional algorithm that is just as fast.<p>[1] Except they're still data structures. They're like, a quantum superposition of data structure and control-flow operator. Which one it is depends on how you look at it.</font></span><p><font size=1><u><a href="reply?id=3829039&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=40></td><td valign=top><center><a id=up_3829734 href="vote?for=3829734&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829734></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=eru">eru</a> 13 hours ago  | <a href="item?id=3829734">link</a></span></div><br>
<span class="comment"><font color=#000000>Yes.  That's why in Haskell programming a common sentiment is that data structures are control structures.  And why we need non-linear data structures for concurrent execution (<a href="http://vimeo.com/6624203" rel="nofollow">http://vimeo.com/6624203</a>).</font></span><p><font size=1><u><a href="reply?id=3829734&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=40></td><td valign=top><center><a id=up_3830427 href="vote?for=3830427&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3830427></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=jpendry">jpendry</a> 9 hours ago  | <a href="item?id=3830427">link</a></span></div><br>
<span class="comment"><font color=#000000>You're getting dangerously close to describing continuations.  And I think it's important to remember that just because a programming language doesn't give you access to a data structure, it doesn't mean it's not using it under the covers.  The call stack is a data structure in all programming languages, only a few provide you with easy access to it.</font></span><p><font size=1><u><a href="reply?id=3830427&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=40></td><td valign=top><center><a id=up_3830526 href="vote?for=3830526&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3830526></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=nandemo">nandemo</a> 9 hours ago  | <a href="item?id=3830526">link</a></span></div><br>
<span class="comment"><font color=#5a5a5a>&#62; O(n log n) algorithm can be made O(n) by replacing eager evaluation with lazy evaluation. My question is, does this count as using the same algorithm or not?<p>I don't think anyone is claiming that it is the same algorithm. Rather, they're claiming that using lazy evaluation gives you more choice of algorithms; and one of these algorithms is faster than the best algorithm under strict evaluation.  I haven't been able to download Bird et. al's paper, but I wouldn't assume it's simply "implementing the same algorithm in Haskell instead of strict ML".</font></span><p><font size=1><u><a href="reply?id=3830526&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=40></td><td valign=top><center><a id=up_3830066 href="vote?for=3830066&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3830066></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=agumonkey">agumonkey</a> 11 hours ago  | <a href="item?id=3830066">link</a></span></div><br>
<span class="comment"><font color=#000000>I think of Data structures as extension, value sets. Function as intension. They are dual to each other, sort of, modulo `some` energy.</font></span><p><font size=1><u><a href="reply?id=3830066&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=0></td><td valign=top><center><a id=up_3828860 href="vote?for=3828860&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3828860></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=rwmj">rwmj</a> 16 hours ago  | <a href="item?id=3828860">link</a></span></div><br>
<span class="comment"><font color=#000000>What's interesting is the implicit assumption that mutating memory is O(1).<p>Unfortunately for <i>real</i> memory (caches, DIMMs, swap etc) that's not true.  It's O(log n) for any non-trivial size of memory, and can have bad (constant?) factors for mutating memory that is shared between processors.<p>Of course functional languages have hidden and not-so-hidden costs too, starting with the garbage collector, but including the time taken to get the larger working set into cache.<p>I'm interested in any studies that look at these total costs in real systems.</font></span><p><font size=1><u><a href="reply?id=3828860&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=40></td><td valign=top><center><a id=up_3829136 href="vote?for=3829136&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829136></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=colanderman">colanderman</a> 15 hours ago  | <a href="item?id=3829136">link</a></span></div><br>
<span class="comment"><font color=#000000>I can't upvote you enough.  Too often programmers conflate "functional programming" with "availability of an O(1) array data type".  The former does not imply lack of the latter, and (as you stated) lack of the latter does not imply the former.<p>The important takeaway I think, is that O(log n) approaches O(1) for all practical values of n.<p>To the downvoters: gee golly, would it hurt you to reply to my comment?  If you think I'm wrong then say <i>why</i>!</font></span><p><font size=1><u><a href="reply?id=3829136&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=80></td><td valign=top><center><a id=up_3829758 href="vote?for=3829758&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829758></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=SamReidHughes">SamReidHughes</a> 13 hours ago  | <a href="item?id=3829758">link</a></span></div><br>
<span class="comment"><font color=#000000>Edit:  O(log n) is clearly worse than O(1) if you just considered an O(log n) array-replacement data structure or hash table-replacement data structure and considered what its performance would be.  I mean, how is this even a question?<p>Also if memory is O(log n) then arrays are O(log n) but BSTs are O((log n)^2).<p>(Edit II: Let's be clear that BSTs are <i>already</i> (log n)^2 if you're counting CPU cycles with O(1) memory access.  They're O(log n) if you count cache misses or comparisons, or you might say they're O((log n)^2) but the first log is base &#60;2 and the second is the logarithm base 2 to the 256th power, so only one really counts.  (The second manifests as a cache line miss if your key comparison walks past the first 32 bytes of your key.))<p>(Actually memory access is at best O(n^(1/3)) thanks to space being three dimensional, but really worse thanks to cooling concerns, and uh, for sufficiently large n, worse because of gravity.)</font></span><p><font size=1><u><a href="reply?id=3829758&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=120></td><td valign=top><center><a id=up_3829918 href="vote?for=3829918&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829918></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=btilly">btilly</a> 12 hours ago  | <a href="item?id=3829918">link</a></span></div><br>
<span class="comment"><font color=#000000><i>O(log n) is clearly worse than O(1) if you just considered an O(log n) array-replacement data structure or hash table-replacement data structure and considered what its performance would be. I mean, how is this even a question?</i><p>Why yes, it is worse in some theoretical manner.  But if you think it is necessarily worse as a practical manner, you have failed to understand the math fully.<p>Suppose that n is 1000 in some toy problem.  Then you scale it up to a huge data set, a billion items!  The log(n) factor only got worse by a factor of 3.  Maybe we didn't go high enough.  Perhaps we can take a large data set.  Let's see, how about all of the data that is produced at CERN's Large Hadron Collider in a year?  That's 15 petabytes.  Now the log(n) factor is a whopping 5.39.<p>In other words while log(n) represents unbounded possible growth, it gets worse in practice by at most a fairly small constant factor.<p>How about O(1)?  O(1) does not mean fast.  It just means constant relative to the data size.  It can be a bad constant, but as long as it is a constant it qualifies as O(1).<p>Let's take a practical example.  Wavelets are cool in lots of ways, but one of the properties people noticed quickly is that doing a basic wavelet transform on a data set with n elements takes time O(n).  Doing a FFT (Fast Fourier Transform) on the same data set takes time O(n log(n)).  Yay, we're faster!<p>But hold on a minute.  The interesting wavelets that we like to use are O(n) with a worse constant than the FFT.  So the FFT tends to be faster on practical data sets.  (There are lots of reasons why one would prefer wavelets over the FFT, but speed of calculation is not generally one of them.)</font></span><p><font size=1><u><a href="reply?id=3829918&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=160></td><td valign=top><center><a id=up_3829955 href="vote?for=3829955&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829955></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=SamReidHughes">SamReidHughes</a> 12 hours ago  | <a href="item?id=3829955">link</a></span></div><br>
<span class="comment"><font color=#000000>&#62;  But if you think it is necessarily worse as a practical manner, you have failed to understand the math fully.<p>Congratulations, I don't think one algorithm is <i>necessarily</i> worse, your entire post was a waste of effort.<p>I mean seriously you just made a post teaching about how logarithmic constants can be low enough that they don't matter in reply to my post which provided an example of that very thing.</font></span><p><font size=1><u><a href="reply?id=3829955&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=200></td><td valign=top><center><a id=up_3830166 href="vote?for=3830166&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3830166></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=btilly">btilly</a> 11 hours ago  | <a href="item?id=3830166">link</a></span></div><br>
<span class="comment"><font color=#000000>I was just explaining the comment that O(log(n)) approaches O(1) for all practical values of n.<p>If you understood this, then why did you start off your post arguing that O(log(n)) was worse than O(1)?<p>Incidentally comparing a logarithmic lookup to a hash lookup, a lot of nosql solutions use hash tables because O(1) is good, right?  For instance look at Apache's Cassandra.  By contrast Google's BigTable uses an ordered data structure which is logarithmic.<p>Guess what, if you've used both, BigTable is better.  Because the extra log cost is irrelevant for the normal use case, and being ordered reduces the operational cost of things like range searches.</font></span><p><font size=1><u><a href="reply?id=3830166&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=240></td><td valign=top><center><a id=up_3830194 href="vote?for=3830194&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3830194></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=SamReidHughes">SamReidHughes</a> 11 hours ago  | <a href="item?id=3830194">link</a></span></div><br>
<span class="comment"><font color=#000000>It's going to be very hard to tell me how I'm wrong when I agree with everything you're saying.</font></span><p><font size=1><u><a href="reply?id=3830194&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=280></td><td valign=top><center><a id=up_3830422 href="vote?for=3830422&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3830422></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=sanderjd">sanderjd</a> 9 hours ago  | <a href="item?id=3830422">link</a></span></div><br>
<span class="comment"><font color=#000000>I found the non-argument that you two just had to be interesting, informative, and entertaining. So don't feel <i>too</i> bad about losing the argumentative-agreement battle - it was great while it lasted!</font></span><p><font size=1><u><a href="reply?id=3830422&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=80></td><td valign=top><center><a id=up_3831411 href="vote?for=3831411&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3831411></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=dan00">dan00</a> 3 hours ago  | <a href="item?id=3831411">link</a></span></div><br>
<span class="comment"><font color=#000000>"I can't upvote you enough. Too often programmers conflate "functional programming" with "availability of an O(1) array data type". The former does not imply lack of the latter, and (as you stated) lack of the latter does not imply the former."<p>Yes, even the pure Haskell has unboxed, mutable O(1) arrays. They're just
a bit hidden for the beginner.</font></span><p><font size=1><u><a href="reply?id=3831411&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=40></td><td valign=top><center><a id=up_3829148 href="vote?for=3829148&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829148></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=mickeyp">mickeyp</a> 15 hours ago  | <a href="item?id=3829148">link</a></span></div><br>
<span class="comment"><font color=#000000>You raise a valid point, and one that a lot of algorithm and data structure textbooks neglect to mention at all; seen purely from a practical perspective, things like locality of data with respect to CPU caches are probably more important than picking a slightly cleverer algorithm/data structure on any modern-day CPU.<p>This once again raises the more subdued or neglected aspect of "premature optimisation": blindly picking an algorithm that looks better on paper may in reality be much slower than a naive algorithm that better matches the characteristics of a computer.<p>Some algorithms are O(1) and you could easily be fooled into believing that they are, indeed, always better than their counterparts that're asymptotically worse than a constant-time; that may not be so, though, if the "constant-time" computations required for non-trivial inputs take longer than say a linear one.</font></span><p><font size=1><u><a href="reply?id=3829148&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=80></td><td valign=top><center><a id=up_3829625 href="vote?for=3829625&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829625></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=bunderbunder">bunderbunder</a> 13 hours ago  | <a href="item?id=3829625">link</a></span></div><br>
<span class="comment"><font color=#000000><i>You raise a valid point, and one that a lot of algorithm and data structure textbooks neglect to mention at all; seen purely from a practical perspective, things like locality of data with respect to CPU caches are probably more important than picking a slightly cleverer algorithm/data structure on any modern-day CPU.</i><p>It may not make it into the basics, but locality of data is absolutely considered in a lot of real work on algorithms and data structures.  For example, the theoretical advantages of hopscotch hashing rest on data locality concerns.  On a hypothetical machine for which memory access always takes the same amount of time, I believe - but have not bothered to prove - that hopscotch hashing and other open addressing techniques should actually be slower than other forms of collision resolution.<p>As for how it affects the question of imperative vs. functional data structure performance, it should tend to favor the imperative ones.  The issue is that purely functional data structures aren't allowed to just grab a block of memory all at once and insert data into it at leisure.  (That would be a form of mutation.)  Instead space has to be allocated as it is needed, in a piecemeal fashion.  As a result, longer-lived and more active purely functional data structures will tend to scatter across the heap over time.</font></span><p><font size=1><u><a href="reply?id=3829625&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=120></td><td valign=top><center><a id=up_3829855 href="vote?for=3829855&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829855></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=carterschonwald">carterschonwald</a> 12 hours ago  | <a href="item?id=3829855">link</a></span></div><br>
<span class="comment"><font color=#000000>Actually, the question then becomes one of trading off sharing vs cache locality. To whit, for many functional languages, memory allocation is essentially just a pointer increment, so there is no a prior reason to not assume that a freshly allocated tree occupies a contiguous region of memory, or at least contiguoudly on each page where it's been allocated.<p>Claims about data structures in the context of how they're allocated in a modern machine architecture / memory model are not the same thing as claims regarding the semantics that are exposed to the user of a programming language.<p>I think you do raise an interesting question, namely what's the layout in memory of a tree like data structure in eg scheme or Haskell, when the tree is built from scratch, but also when updated and a garbage collection run has compacted the data.  Oooo, I totally want to investigate this now! (I don't have the time to, but I totally want to.)<p>Point being, depending on the memory management semantics for heap data in a programming language, any claims about locslity happening for tree data may or may not happen.<p>This actually raises a really beautiful question: what is the complextity of any garbage collection algorithm that exactly or approximately maximizes the locality of tree or directed graph like data structures in memory? I want to speculate that perhaps the data structures which people find fast in practice are actually in some sense enabling the gc to preserve locality.  I wonder how optimally the layout can be when only linear or nearly linear time algorithms are considered!</font></span><p><font size=1><u><a href="reply?id=3829855&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=160></td><td valign=top><center><a id=up_3829912 href="vote?for=3829912&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829912></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=bunderbunder">bunderbunder</a> 12 hours ago  | <a href="item?id=3829912">link</a></span></div><br>
<span class="comment"><font color=#000000><i>there is no a prior reason to not assume that a freshly allocated tree occupies a contiguous region of memory</i><p>But I'm sure you'll also agree that a freshly allocated tree probably hasn't been alive for long and hasn't seen much activity. :)</font></span><p><font size=1><u><a href="reply?id=3829912&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=200></td><td valign=top><center><a id=up_3830041 href="vote?for=3830041&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3830041></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=carterschonwald">carterschonwald</a> 11 hours ago  | <a href="item?id=3830041">link</a></span></div><br>
<span class="comment"><font color=#000000>Very true! Hence the part where I'm thinking out loud regarding how the gc ordering of data after a copy/compact (in the one generation gc) impacts locality. And I suppose this also applies to more modern gc setups like multiple generations or parallel collection. Is there some notion of data locality that we can formalize and analyze for at least simplified versions of modern memory hierarchy? How well do standard gc algorithms perform when analyzed thusly?  How efficient can each style of gc algorithm be if it's designed to optimize locality?<p>It clearly has some relation to optimal caching optimization problems, but with more explicit interdependencies. It's also seemingly both an online and offline problem in terms of how to analyze it, but I need to think on it more</font></span><p><font size=1><u><a href="reply?id=3830041&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=120></td><td valign=top><center><a id=up_3829805 href="vote?for=3829805&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829805></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=eru">eru</a> 12 hours ago  | <a href="item?id=3829805">link</a></span></div><br>
<span class="comment"><font color=#000000>&#62; As for how it affects the question of imperative vs. functional data structure performance, it should tend to favor the imperative ones. The issue is that purely functional data structures aren't allowed to just grab a block of memory all at once and insert data into it at leisure. (That would be a form of mutation.) Instead space has to be allocated as it is needed, in a piecemeal fashion. As a result, longer-lived and more active purely functional data structures will tend to scatter across the heap over time.<p>It's a bit more complicated than that.  To talk about runtime (and memory) of an algorithm or a data structure, you need to have a machine model.  If you look at what happens in, say, Haskell: We write our algorithms in a purely functional way, but we are interested in their runtime after compilation into native code.  The compiler is free to do any transformation that preserves correctness, so if it's smart enough, it might grab a block of memory all at once.  Or if it can prove that data won't be used again, it can safely change variables in place.  (The language Clean even has type system support for that kind of mutation.  (See <a href="https://en.wikipedia.org/wiki/Linear_type_system)" rel="nofollow">https://en.wikipedia.org/wiki/Linear_type_system)</a>)<p>You might be able to analyse functional algorithms with a machine model that doesn't involve compilation to some form of imperative code.  Perhaps by counting reductions in lambda calculus.  But then it's hard to compare that to asymptotic performance of imperative algorithms.</font></span><p><font size=1><u><a href="reply?id=3829805&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=160></td><td valign=top><center><a id=up_3829878 href="vote?for=3829878&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829878></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=bunderbunder">bunderbunder</a> 12 hours ago  | <a href="item?id=3829878">link</a></span></div><br>
<span class="comment"><font color=#000000>Absolutely.  But many of those hypothetical compiler transformations would only work if the code isn't really taking advantage some of the most important advantages of purely functional data structures.  Persistence, for example.<p>That, and in a sense it's side-stepping the theoretical issue more than anything else.  A compiler that's smart enough to find spots where a performance improvement can safely be gained by replacing a pure structure with a mutable one in the background doesn't really counter the idea that impure structures can be faster so much as acknowledge it.</font></span><p><font size=1><u><a href="reply?id=3829878&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=200></td><td valign=top><center><a id=up_3829903 href="vote?for=3829903&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829903></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=eru">eru</a> 12 hours ago  | <a href="item?id=3829903">link</a></span></div><br>
<span class="comment"><font color=#000000>Yes.  And instead of comparing purely functional vs imperative languages, we could sidestep those issues by comparing persistent vs ephemeral data structures, regardless of language.<p>(For an example of the distinction--straight out of Okasaki's book: the standard purely-functional queue implementation as two linked lists has O(1) amortized runtime in adding and removing only if used `single-threaded' / ephemeral.  As a persistent data structure you get a worst case of O(n).)</font></span><p><font size=1><u><a href="reply?id=3829903&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=240></td><td valign=top><center><a id=up_3829948 href="vote?for=3829948&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829948></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=bunderbunder">bunderbunder</a> 12 hours ago  | <a href="item?id=3829948">link</a></span></div><br>
<span class="comment"><font color=#000000>Sure.  Honestly, that's how I had been interpreting it from the beginning.  The SO question seems to be specifically talking about algorithms and not programming languages.</font></span><p><font size=1><u><a href="reply?id=3829948&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=80></td><td valign=top><center><a id=up_3829268 href="vote?for=3829268&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829268></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=mjn">mjn</a> 14 hours ago  | <a href="item?id=3829268">link</a></span></div><br>
<span class="comment"><font color=#000000>Even the "on-paper" analyses can take some of this into account, with algorithm analysis models that take into account cache and NUMA architectures. But, people do tend to just look at the classic big-O analyses, which may not be right.<p>In a way that kind of dependence isn't new. If you look at old versions of Knuth's TAOCP, for many algorithms he has separate analyses for the in-RAM case and the on-tape case, using two different memory models, and in some cases which algorithm is better differs a lot between the two cases. But it may be that algorithms reference books are doing a worse job keeping up now than they used to.</font></span><p><font size=1><u><a href="reply?id=3829268&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=120></td><td valign=top><center><a id=up_3829431 href="vote?for=3829431&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829431></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=mickeyp">mickeyp</a> 14 hours ago  | <a href="item?id=3829431">link</a></span></div><br>
<span class="comment"><font color=#000000>Knuth is probably the only (major) author that I can think of off-hand who took the application of his algorithms and data structures so seriously (going so far as to invent his own little RISC instruction set, MIX (later MMIX.) in a book that concerned itself with the theory as aswell as the practical aspects. Most books focus almost exclusively on one or the other.<p>I think aside from things like cache coherency, memory models and things like SIMD, computer scientists will have to think of their algorithms (or at least cover it when they write books for undergrads or professionals) in terms of commutativity and associativity so people can see how parallelisable an algorithm is or can be.</font></span><p><font size=1><u><a href="reply?id=3829431&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=120></td><td valign=top><center><a id=up_3829498 href="vote?for=3829498&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829498></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=rwmj">rwmj</a> 14 hours ago  | <a href="item?id=3829498">link</a></span></div><br>
<span class="comment"><font color=#000000>There are a few good books listed here:<p><a href="http://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Books.html" rel="nofollow">http://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Books...</a><p>which do fully analyse the hardware.</font></span><p><font size=1><u><a href="reply?id=3829498&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=40></td><td valign=top><center><a id=up_3829415 href="vote?for=3829415&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829415></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=scott_s">scott_s</a> 14 hours ago  | <a href="item?id=3829415">link</a></span></div><br>
<span class="comment"><font color=#000000>Bjarne Stroustrup hit a similar point in his recent Going Native keynote (skip to 45 minutes in): <a href="http://www.youtube.com/watch?v=OB-bdWKwXsU" rel="nofollow">http://www.youtube.com/watch?v=OB-bdWKwXsU</a></font></span><p><font size=1><u><a href="reply?id=3829415&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=40></td><td valign=top><center><a id=up_3829347 href="vote?for=3829347&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829347></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=adrianN">adrianN</a> 14 hours ago  | <a href="item?id=3829347">link</a></span></div><br>
<span class="comment"><font color=#000000>To my knowledge, mutating memory is asymptotically the same as just reading it. Hence any asymptotic slowdown purely functional language have will translate, regardless of how long memory operations take.</font></span><p><font size=1><u><a href="reply?id=3829347&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=80></td><td valign=top><center><a id=up_3829489 href="vote?for=3829489&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829489></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=rwmj">rwmj</a> 14 hours ago  | <a href="item?id=3829489">link</a></span></div><br>
<span class="comment"><font color=#000000>Are you sure?  Writing to memory that might be shared between processors certainly isn't the same as reading, and so a parallel algorithm (and what isn't, these days?) should do better sharing lots of read-only data, rather than mutating shared state.</font></span><p><font size=1><u><a href="reply?id=3829489&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=40></td><td valign=top><center><a id=up_3831672 href="vote?for=3831672&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3831672></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=brazzy">brazzy</a> 1 hour ago  | <a href="item?id=3831672">link</a></span></div><br>
<span class="comment"><font color=#000000>Actually, memory access is O(n^1/3) if you want to get really real...</font></span><p><font size=1><u><a href="reply?id=3831672&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=0></td><td valign=top><center><a id=up_3828448 href="vote?for=3828448&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3828448></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=jerf">jerf</a> 17 hours ago  | <a href="item?id=3828448">link</a></span></div><br>
<span class="comment"><font color=#000000>In addition to the great answer mentioned there, it should also be mentioned that every language referred to as functional still has the ability to use mutable arrays in some manner. Yes, even Haskell, it just requires you to carefully control where the mutation is used.</font></span><p><font size=1><u><a href="reply?id=3828448&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=40></td><td valign=top><center><a id=up_3828972 href="vote?for=3828972&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3828972></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=greiskul">greiskul</a> 15 hours ago  | <a href="item?id=3828972">link</a></span></div><br>
<span class="comment"><font color=#000000>Yes, I think the great lesson we can take from functional programming is not of being functional 100% of the time, but of not having mutable global state. Your inner loops, local variables, etc. can all be mutable and nobody will care, as long as your interface is functional.</font></span><p><font size=1><u><a href="reply?id=3828972&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=80></td><td valign=top><center><a id=up_3830797 href="vote?for=3830797&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3830797></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=fpgeek">fpgeek</a> 7 hours ago  | <a href="item?id=3830797">link</a></span></div><br>
<span class="comment"><font color=#000000>Yes and no. It is true that what matters most of the time is a functional interface, not how that is achieved. That being said, it is much harder to successfully implement a functional interface if you are using side-effects willy-nilly inside.</font></span><p><font size=1><u><a href="reply?id=3830797&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=0></td><td valign=top><center><a id=up_3828832 href="vote?for=3828832&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3828832></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=chimeracoder">chimeracoder</a> 16 hours ago  | <a href="item?id=3828832">link</a></span></div><br>
<span class="comment"><font color=#000000><i>Purely</i> functional programming? I dunno, what's the asymptotic cost of flipping a switch on a box and watching it warm up[1]?<p>Nobody advocates <i>purely</i> functional programming, at least not in the <i>implementation</i> of a language itself, but it can be an incredibly valuable model for a language <i>interface</i>. In the same way, a stack (or better yet, semi-infinite tape) can be an incredibly valuable model for a language interface at well. Functional programming is, by definition, just another way of saying that your model of computation is based on the lambda calculus instead of a Turing machine. You can just as easily ask what the asymptotic 'cost' of using a non-random access Turing machine is.<p>If people really want a functional language with a <i>purely</i> functional implementation, then you have to say 'farewell' to even things like tail-recursion, since you can make the argument that converting recursion to a loop is anti-lambda calculus.<p><i></i>EDIT<i></i>: It seems there's some confusion over the distinction between implementing a purely functional algorithm in a language and implementing an language in a purely functional manner. My point is that, even if you implement your algorithm in a purely functional manner, if your compiler is simply converting it to a (well-optimized) imperative, destructive code behind-the-scenes and making it <i>appear</i> to be immutable, then this entire question is rather beside the point.
[1] (With apologies to Simon Peyton Jones).</font></span><p><font size=1><u><a href="reply?id=3828832&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=40></td><td valign=top><center><a id=up_3829124 href="vote?for=3829124&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829124></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=aplusbi">aplusbi</a> 15 hours ago  | <a href="item?id=3829124">link</a></span></div><br>
<span class="comment"><font color=#000000>If it were really purely functional it wouldn't even warm up.  When you compile your code, referential transparency would result in your executable being nothing more than the final value produced by your code.</font></span><p><font size=1><u><a href="reply?id=3829124&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=80></td><td valign=top><center><a id=up_3829246 href="vote?for=3829246&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829246></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=chimeracoder">chimeracoder</a> 15 hours ago  | <a href="item?id=3829246">link</a></span></div><br>
<span class="comment"><font color=#000000>I've had this debate before :-)
<a href="http://twitter.theinfo.org/171745809686200320#id171799214685306883" rel="nofollow">http://twitter.theinfo.org/171745809686200320#id171799214685...</a><p>If we want to get <i>really</i> pedantic, I guess we could say that we're using one Turing machine (our computer) to emulate another equivalent model of computation (the lambda calculus), so the computer warms up because the Turing Machine is an inefficient emulator of the instantaneously-evaluating lambda calculus evaluator. But the lambda calculus evaluator doesn't change state.<p>...so, what we <i>really</i> need is to wait for someone to invent a lambda calculus evaluator. Then we wouldn't need to emulate them with these silly Turing machines, and we could get instant evaluations of our programs based solely on -reductions, with no side-effects (thermal side-effects included). That would put an end to this debate!<p>(Furthermore, who cares if P=NP if -reductions can be evaluated 'directly' instead of being emulated by these obsolete Turing machines? Even the hardest decidable problems would be solved instantaneously!)</font></span><p><font size=1><u><a href="reply?id=3829246&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=120></td><td valign=top><center><a id=up_3829474 href="vote?for=3829474&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829474></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=skew">skew</a> 14 hours ago  | <a href="item?id=3829474">link</a></span></div><br>
<span class="comment"><font color=#000000>If you were really pedantic, you'd make a distinction between executing beta-reductions directly, and executing them in zero time.<p>Here's hardware that does the first <a href="http://www.cs.york.ac.uk/fp/reduceron/" rel="nofollow">http://www.cs.york.ac.uk/fp/reduceron/</a></font></span><p><font size=1><u><a href="reply?id=3829474&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=160></td><td valign=top><center><a id=up_3829662 href="vote?for=3829662&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829662></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=chimeracoder">chimeracoder</a> 13 hours ago  | <a href="item?id=3829662">link</a></span></div><br>
<span class="comment"><font color=#000000>Yeah, in case it wasn't clear, by my second reply I was just being facetious.<p>Pedantry is infinitely recursive, with no base case in sight and no tail optimization - my tolerance/stack for these things overflows rather quickly. :-)<p>But thanks for the link! I'll be sure to check it out later; it looks interesting.</font></span><p><font size=1><u><a href="reply?id=3829662&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=80></td><td valign=top><center><a id=up_3829485 href="vote?for=3829485&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829485></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=Dn_Ab">Dn_Ab</a> 14 hours ago  | <a href="item?id=3829485">link</a></span></div><br>
<span class="comment"><font color=#000000>Depends on the program. If it were computing all the twin primes it would warm up. Here the input would be round about - how long you kept the program running. And since each time a bit was erased   or a non-not operation was done &#62; klog2(T) J must be dissipated, a sufficiently sophisticated being could use the timings and heat signature to infer the state and value to extract a runnning output. Another output you could more easily get is that if it started to cool down and you had a notion to return an output after 10^(absurdly large number) successive failures then you have  a pretty good confidence bound on the falsity of the conjecture..</font></span><p><font size=1><u><a href="reply?id=3829485&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=40></td><td valign=top><center><a id=up_3829206 href="vote?for=3829206&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829206></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=skew">skew</a> 15 hours ago  | <a href="item?id=3829206">link</a></span></div><br>
<span class="comment"><font color=#000000>&#62; You can just as easily ask what the asymptotic 'cost' of using a non-random access Turing machine is.<p>Precisely. For example, you can ask whether the best algorithm for some problem on a 1-tape Turing machine is asymptotically slower than the best algorithm on a 2-tape Turing machine.</font></span><p><font size=1><u><a href="reply?id=3829206&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=80></td><td valign=top><center><a id=up_3829305 href="vote?for=3829305&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829305></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=chimeracoder">chimeracoder</a> 14 hours ago  | <a href="item?id=3829305">link</a></span></div><br>
<span class="comment"><font color=#000000>Okay, great, but what's the point? At the end of the day, we're not running anything on Turing machines; we're running them on computers, which are <i>almost</i> but not quite the same thing. Remember that the Turing machine is intended as a model for <i>algorithms</i> and <i>not</i> a model for the actual execution process/environment of those algorithms. (Again, a distinction that's irrelevant when discussing TMs in most contexts, so it's generally brushed over).<p>And at the end of the day, my original point stands: we're talking about writing purely functional code, but if the code is being compiled by a compiler that takes advantage of non-functional code optimization (and as far as I know, nearly all general-purpose compilers do to some degree), then it doesn't make sense to compare the functional code vs. non-functional code. Even if it <i>appears</i> that data structures are immutable, if the compiler is mutating them behind-the-scenes, it all depends on how well-optimized the compiler is, and that's a very different discussion than the one implied by the title.</font></span><p><font size=1><u><a href="reply?id=3829305&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=120></td><td valign=top><center><a id=up_3829460 href="vote?for=3829460&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829460></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=skew">skew</a> 14 hours ago  | <a href="item?id=3829460">link</a></span></div><br>
<span class="comment"><font color=#000000>Do you ever try to understand the asymptotic performance of a program in terms of the source code, or only read compiled binaries?<p>You can describe the performance of a functional program in something like asymptotic number of reductions. You can write compilers that will run those programs on real hardware in time related to the performance bound you get by working in the source model. Also, perfect optimization is undecidable. So, the question is whether there are problems where you necessarily lost asymptotic performance by working with a purely functional programming language.</font></span><p><font size=1><u><a href="reply?id=3829460&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=160></td><td valign=top><center><a id=up_3829635 href="vote?for=3829635&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829635></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=chimeracoder">chimeracoder</a> 13 hours ago  | <a href="item?id=3829635">link</a></span></div><br>
<span class="comment"><font color=#000000>&#62; Also, perfect optimization is undecidable.<p>In the general case, not necessarily for any subset, and this would apply to to non-functional algorithms as well. Just because I write a 'simple' for/while loop, that doesn't <i>necessarily</i> imply anything about the actual<p>At the end of the day, an algorithm is simply an unambiguous, executable, and terminating <i>specification</i> of a way to solve a problem. We may implement an algorithm by creating a physical or virtual machine that conforms to that specification, but even abstractions like Turing machines and lambda calculi are just attempts at creating a deterministic model that we can manipulate in order to understand the algorithm, which is an abstract specification.<p>The problem is that in general, algorithms are not specified in perfect detail, or they are implemented in ways that are logicially equivalent, but not exactly equivalent, with the specification. (Or oftentimes both). When dealing with any specific algorithm, we can usually infer that analyses of one will translate to the other, but in the general case, this assumption is murky.<p>The difference between functional and non-functional for an <i>algorithm</i> happens at the level of the specification, which is different from the level of the implementation. The translation from one to the other involves optimization. And if perfect optimization is undecidable, as you point out, so is the question of whether a perfectly optimized implementation of one <i>class</i> of algorithms is 'better' than a perfectly optimized implementation of another class, since we're no longer dealing with individual instances.</font></span><p><font size=1><u><a href="reply?id=3829635&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=0></td><td valign=top><center><a id=up_3829331 href="vote?for=3829331&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829331></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=jacquesm">jacquesm</a> 14 hours ago  | <a href="item?id=3829331">link</a></span></div><br>
<span class="comment"><font color=#000000>There is a very large difference between what you write and what your computer actually does.<p>Something may look like it is O(1) but in reality it may be <i>much</i> slower. The only way to find out is to put the program through its paces with various values for 'n', and to measure the time it took to compute the answer.<p>Prepare to be surprised.</font></span><p><font size=1><u><a href="reply?id=3829331&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=0></td><td valign=top><center><a id=up_3829736 href="vote?for=3829736&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829736></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=scscsc">scscsc</a> 13 hours ago  | <a href="item?id=3829736">link</a></span></div><br>
<span class="comment"><font color=#000000>There seem to be a lot of people confusing complexity with practical issues.<p>I think we can agree that:
1) you can write most software in a functional programming language without any (noticeable) loss of performance
2) there will always be software (video encoders, HPC stuff) that will always be done in Fortran/C :D<p>However, I think the question is very interesting from a <i>theoretical</i> point of view: if you model imperative algorithm as programs for a RAM machine (good theoretical approximation of real single core computers) and purely functional algorithms as programs for a TBI machine (good theoretical approximation of a hypothetical machine executing pure algorithms), is there a problem for which a faster solution exists for a RAM than for a TBI?<p>Note. RAM machines exist and are standard in literature; to my knowledge TBI (to-be-invented) machines do not (yet) exist.</font></span><p><font size=1><u><a href="reply?id=3829736&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=40></td><td valign=top><center><a id=up_3829839 href="vote?for=3829839&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829839></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=eru">eru</a> 12 hours ago  | <a href="item?id=3829839">link</a></span></div><br>
<span class="comment"><font color=#000000>I agree.<p>&#62; 2) there will always be software (video encoders, HPC stuff) that will always be done in Fortran/C :D<p>Alas, yes.  But there are efforts underway to challenge that.  Theoretically, your compiler should be able to do a better job, if it knows more about your intent.  I'm learning about some techniques for really high performance Haskell.  E.g.:<p>* Iteratee/Enumeratee: <a href="http://okmij.org/ftp/Streams.html" rel="nofollow">http://okmij.org/ftp/Streams.html</a><p>* Stream fusion: <a href="https://donsbot.wordpress.com/2008/06/04/haskell-as-fast-as-c-working-at-a-high-altitude-for-low-level-performance/" rel="nofollow">https://donsbot.wordpress.com/2008/06/04/haskell-as-fast-as-...</a><p>The C equivalent of stream fusion would be loop fusion: if you have to for loops one after another iterating over the same array, you could fuse them into one loop.  That same trick works in Haskell, even more often and with more benefits.</font></span><p><font size=1><u><a href="reply?id=3829839&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=0></td><td valign=top><center><a id=up_3828962 href="vote?for=3828962&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3828962></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=ww520">ww520</a> 15 hours ago  | <a href="item?id=3828962">link</a></span></div><br>
<span class="comment"><font color=#000000>Lazy evaluation probably adds more unintuitive cost.  E.g. Haskell uses <i>thunk</i> to track yet-to-be-evaluated values.  These thunks take up memory and take up CPU cost.  For lazily evaluated function calls (or expression), each call has a thunk created and it will be evaluated later when needed.  For deep recursive calls the thunk link can be very long.<p>Lazy evaluation is great in that the actual computation might not needed at the end but there are non-trivial cost to track and evaluate all the thunks.<p>It's an extra dimension to consider when comparing the cost of doing simple computation with strict evaluation vs utilizing lazy evaluation with the added thunking cost.</font></span><p><font size=1><u><a href="reply?id=3828962&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=40></td><td valign=top><center><a id=up_3829830 href="vote?for=3829830&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829830></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=lnmx">lnmx</a> 12 hours ago  | <a href="item?id=3829830">link</a></span></div><br>
<span class="comment"><font color=#000000>I learned a lot about this from Edward Z. Yang's blog:<p><a href="http://blog.ezyang.com/2011/06/pinpointing-space-leaks-in-big-programs/" rel="nofollow">http://blog.ezyang.com/2011/06/pinpointing-space-leaks-in-bi...</a><p><a href="http://blog.ezyang.com/2011/04/the-haskell-heap/" rel="nofollow">http://blog.ezyang.com/2011/04/the-haskell-heap/</a></font></span><p><font size=1><u><a href="reply?id=3829830&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr><td><table border=0><tr><td><img src="http://ycombinator.com/images/s.gif" height=1 width=0></td><td valign=top><center><a id=up_3829410 href="vote?for=3829410&dir=up&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30"><img src="http://ycombinator.com/images/grayarrow.gif" border=0 vspace=3 hspace=2></a><span id=down_3829410></span></center></td><td class="default"><div style="margin-top:2px; margin-bottom:-10px; "><span class="comhead"><a href="user?id=jwatte">jwatte</a> 14 hours ago  | <a href="item?id=3829410">link</a></span></div><br>
<span class="comment"><font color=#000000>I've had some whoppers in Erlang, where "garbage generated" ends up costing N-squared, even though the operations were just N. In fact, the garbage collection footprint turns out to be one of the most significant markers for functional performance in my experience. Erlang's per-process heaps help here, because each heap can typically be tiny, but it's still something that needs to be managed as closely as, say, stale pointers in C++.</font></span><p><font size=1><u><a href="reply?id=3829410&whence=%69%74%65%6d%3f%69%64%3d%33%38%32%38%33%33%30">reply</a></u></font></td></tr></table></td></tr><tr style="height:10px"></tr><tr><td class="title"><a href="/x?fnid=r6DCdmFvVu" rel="nofollow">More</a></td></tr></table><br><br>
</td></tr><tr><td><img src="http://ycombinator.com/images/s.gif" height=10 width=0><table width="100%" cellspacing=0 cellpadding=1><tr><td bgcolor=#ff6600></td></tr></table><br>
<center><span class="yclinks"><a href="lists">Lists</a> | <a href="rss">RSS</a> | <a href="http://ycombinator.com/bookmarklet.html">Bookmarklet</a> | <a href="http://ycombinator.com/newsguidelines.html">Guidelines</a> | <a href="http://ycombinator.com/newsfaq.html">FAQ</a> | <a href="http://ycombinator.com/newsnews.html">News News</a> | <a href="item?id=363">Feature Requests</a> | <a href="http://ycombinator.com">Y Combinator</a> | <a href="http://ycombinator.com/apply.html">Apply</a> | <a href="http://ycombinator.com/lib.html">Library</a></span><br><br>
<form method=get action="http://www.hnsearch.com/search#request/all">Search: <input type=text name="q" value="" size=17></form><br>
</center></td></tr></table></center></body></html>